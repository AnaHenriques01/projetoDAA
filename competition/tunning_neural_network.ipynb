{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, Dropout\n",
    "from tensorflow.python.keras.losses import SparseCategoricalCrossentropy\n",
    "from sklearn.model_selection import GridSearchCV, KFold, train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "print(tf.__version__)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Carregamento de dados"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "TRAINING_DATASET_SOURCE = 'training_data.csv'\n",
    "TEST_DATASET_SOURCE = 'test_data.csv'\n",
    "\n",
    "train_df = pd.read_csv(TRAINING_DATASET_SOURCE)\n",
    "test_df = pd.read_csv(TEST_DATASET_SOURCE)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# SEED utilizada"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "SEED = 101"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preparação dos dados"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "categorical_to_numerical = {\n",
    "    'avg_rain': {\n",
    "        'Sem Chuva': 0,\n",
    "        'chuva fraca': 1,\n",
    "        'chuva moderada': 2,\n",
    "        'chuva forte': 3\n",
    "    },\n",
    "    'luminosity': {\n",
    "        'LOW_LIGHT': 0,\n",
    "        'LIGHT': 1,\n",
    "        'DARK': 2,\n",
    "    }\n",
    "}\n",
    "\n",
    "incidents_to_numerical = {\n",
    "    'incidents': {\n",
    "        'None': 0,\n",
    "        'Low': 1,\n",
    "        'Medium': 2,\n",
    "        'High': 3,\n",
    "        'Very_High': 4,\n",
    "    }\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def neural_network_data_preparation(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    dropped_columns = ['city_name', 'magnitude_of_delay', 'avg_precipitation']\n",
    "\n",
    "    prep_df = df.drop(dropped_columns, axis=1)\n",
    "\n",
    "    ### Extrair a hora e dia da semana da feature 'record_date'\n",
    "    record_date = pd.DatetimeIndex(prep_df['record_date'])\n",
    "\n",
    "    prep_df['record_date_hour'] = record_date.hour\n",
    "    prep_df['record_date_day'] = record_date.day\n",
    "    prep_df['record_date_month'] = record_date.month\n",
    "    prep_df['record_date_weekday'] = record_date.weekday\n",
    "\n",
    "    prep_df.drop(columns=['record_date'], inplace=True)\n",
    "\n",
    "    ### Quantificar a feature 'affected_roads' para o número único de estradas afetadas\n",
    "    road_quantity = []\n",
    "    for line in prep_df['affected_roads']:\n",
    "        res = set(str(line).split(','))\n",
    "        res2 = [elem for elem in res if elem != '']\n",
    "        count = len(res2)\n",
    "        road_quantity.append(count)\n",
    "\n",
    "    prep_df['affected_roads'] = road_quantity\n",
    "\n",
    "    prep_df.replace(categorical_to_numerical, inplace=True)\n",
    "\n",
    "    ### Target\n",
    "    if 'incidents' in prep_df.columns:\n",
    "        prep_df.replace(incidents_to_numerical, inplace=True)\n",
    "\n",
    "    return prep_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "X = neural_network_data_preparation(train_df)\n",
    "y = X['incidents']\n",
    "\n",
    "X.drop(columns=['incidents'], inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "scaler_X = MinMaxScaler(feature_range=(0, 1)).fit(X)\n",
    "X_scaled = pd.DataFrame(scaler_X.transform(X[X.columns]), columns=X.columns)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=SEED)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Construção da estrutura da rede neuronal"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "!pip install -q -U keras-tuner"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import keras_tuner as kt\n",
    "\n",
    "\n",
    "def build_model(hp):\n",
    "    # Create a sequential model (with three layers - last one is the output)\n",
    "    model = keras.Sequential()\n",
    "\n",
    "    input_hp_units = hp.Int('units', min_value=5, max_value=200, step=5)\n",
    "\n",
    "    model.add(Dense(input_hp_units, input_dim=12, activation='relu'))\n",
    "\n",
    "    hp_units = hp.Int('units', min_value=5, max_value=200, step=5)\n",
    "\n",
    "    hp_activation = hp.Choice(\"activation\", [\"relu\", \"tanh\", 'sigmoid'])\n",
    "\n",
    "    model.add(Dense(hp_units, activation=hp_activation))\n",
    "\n",
    "    # Output Layer\n",
    "    model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-1, 1e-2, 1e-3, 1e-4, 1e-5])\n",
    "\n",
    "    # Model compilation\n",
    "    model.compile(loss=SparseCategoricalCrossentropy(), optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "tuner = kt.RandomSearch(\n",
    "    hypermodel=build_model,\n",
    "    objective=\"accuracy\",\n",
    "    max_trials=3,\n",
    "    executions_per_trial=2,\n",
    "    overwrite=True,\n",
    "    directory=\"trials\",\n",
    "    project_name=\"competition\",\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1 Complete [00h 00m 06s]\n",
      "accuracy: 0.5791249871253967\n",
      "\n",
      "Best accuracy So Far: 0.5791249871253967\n",
      "Total elapsed time: 00h 00m 06s\n",
      "\n",
      "Search: Running Trial #2\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "135               |130               |units\n",
      "tanh              |sigmoid           |activation\n",
      "0.0001            |0.001             |learning_rate\n",
      "\n",
      "Epoch 1/500\n",
      "125/125 [==============================] - 1s 2ms/step - loss: 1.4973 - accuracy: 0.4510 - val_loss: 1.3166 - val_accuracy: 0.5170\n",
      "Epoch 2/500\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 1.2133 - accuracy: 0.5630 - val_loss: 1.2019 - val_accuracy: 0.5640\n",
      "Epoch 3/500\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 1.1476 - accuracy: 0.5770 - val_loss: 1.1644 - val_accuracy: 0.5700\n",
      "Epoch 4/500\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 1.1321 - accuracy: 0.5815 - val_loss: 1.1649 - val_accuracy: 0.5760\n",
      "Epoch 5/500\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 1.1191 - accuracy: 0.5810 - val_loss: 1.1487 - val_accuracy: 0.5750\n",
      "Epoch 6/500\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 1.1042 - accuracy: 0.5860 - val_loss: 1.1394 - val_accuracy: 0.5790\n",
      "Epoch 7/500\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 1.0970 - accuracy: 0.5867 - val_loss: 1.1210 - val_accuracy: 0.5780\n",
      "Epoch 8/500\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 1.0906 - accuracy: 0.5867 - val_loss: 1.1209 - val_accuracy: 0.5710\n",
      "Epoch 9/500\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 1.0795 - accuracy: 0.5907 - val_loss: 1.1140 - val_accuracy: 0.5920\n",
      "Epoch 10/500\n",
      "  1/125 [..............................] - ETA: 0s - loss: 1.2031 - accuracy: 0.5938"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [12], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m stop_early \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mkeras\u001B[38;5;241m.\u001B[39mcallbacks\u001B[38;5;241m.\u001B[39mEarlyStopping(monitor\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mval_loss\u001B[39m\u001B[38;5;124m'\u001B[39m, patience\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m5\u001B[39m)\n\u001B[1;32m----> 3\u001B[0m tuner\u001B[38;5;241m.\u001B[39msearch(X, y, epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m500\u001B[39m, validation_split\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.2\u001B[39m, callbacks\u001B[38;5;241m=\u001B[39m[stop_early])\n\u001B[0;32m      5\u001B[0m \u001B[38;5;66;03m# Get the optimal hyperparameters\u001B[39;00m\n\u001B[0;32m      6\u001B[0m best_hps \u001B[38;5;241m=\u001B[39m tuner\u001B[38;5;241m.\u001B[39mget_best_hyperparameters(num_trials\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)[\u001B[38;5;241m0\u001B[39m]\n",
      "File \u001B[1;32m~\\.conda\\envs\\DAA\\lib\\site-packages\\keras_tuner\\engine\\base_tuner.py:183\u001B[0m, in \u001B[0;36mBaseTuner.search\u001B[1;34m(self, *fit_args, **fit_kwargs)\u001B[0m\n\u001B[0;32m    180\u001B[0m     \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[0;32m    182\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_trial_begin(trial)\n\u001B[1;32m--> 183\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrun_trial(trial, \u001B[38;5;241m*\u001B[39mfit_args, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfit_kwargs)\n\u001B[0;32m    184\u001B[0m \u001B[38;5;66;03m# `results` is None indicates user updated oracle in `run_trial()`.\u001B[39;00m\n\u001B[0;32m    185\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m results \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32m~\\.conda\\envs\\DAA\\lib\\site-packages\\keras_tuner\\engine\\tuner.py:295\u001B[0m, in \u001B[0;36mTuner.run_trial\u001B[1;34m(self, trial, *args, **kwargs)\u001B[0m\n\u001B[0;32m    293\u001B[0m     callbacks\u001B[38;5;241m.\u001B[39mappend(model_checkpoint)\n\u001B[0;32m    294\u001B[0m     copied_kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcallbacks\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m callbacks\n\u001B[1;32m--> 295\u001B[0m     obj_value \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_and_fit_model(trial, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mcopied_kwargs)\n\u001B[0;32m    297\u001B[0m     histories\u001B[38;5;241m.\u001B[39mappend(obj_value)\n\u001B[0;32m    298\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m histories\n",
      "File \u001B[1;32m~\\.conda\\envs\\DAA\\lib\\site-packages\\keras_tuner\\engine\\tuner.py:222\u001B[0m, in \u001B[0;36mTuner._build_and_fit_model\u001B[1;34m(self, trial, *args, **kwargs)\u001B[0m\n\u001B[0;32m    220\u001B[0m hp \u001B[38;5;241m=\u001B[39m trial\u001B[38;5;241m.\u001B[39mhyperparameters\n\u001B[0;32m    221\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_try_build(hp)\n\u001B[1;32m--> 222\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhypermodel\u001B[38;5;241m.\u001B[39mfit(hp, model, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    223\u001B[0m tuner_utils\u001B[38;5;241m.\u001B[39mvalidate_trial_results(\n\u001B[0;32m    224\u001B[0m     results, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moracle\u001B[38;5;241m.\u001B[39mobjective, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mHyperModel.fit()\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    225\u001B[0m )\n\u001B[0;32m    226\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m results\n",
      "File \u001B[1;32m~\\.conda\\envs\\DAA\\lib\\site-packages\\keras_tuner\\engine\\hypermodel.py:140\u001B[0m, in \u001B[0;36mHyperModel.fit\u001B[1;34m(self, hp, model, *args, **kwargs)\u001B[0m\n\u001B[0;32m    116\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfit\u001B[39m(\u001B[38;5;28mself\u001B[39m, hp, model, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m    117\u001B[0m     \u001B[38;5;124;03m\"\"\"Train the model.\u001B[39;00m\n\u001B[0;32m    118\u001B[0m \n\u001B[0;32m    119\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    138\u001B[0m \u001B[38;5;124;03m        If return a float, it should be the `objective` value.\u001B[39;00m\n\u001B[0;32m    139\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 140\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m model\u001B[38;5;241m.\u001B[39mfit(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\.conda\\envs\\DAA\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     62\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m     63\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 64\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m     65\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:  \u001B[38;5;66;03m# pylint: disable=broad-except\u001B[39;00m\n\u001B[0;32m     66\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[1;32m~\\.conda\\envs\\DAA\\lib\\site-packages\\keras\\engine\\training.py:1409\u001B[0m, in \u001B[0;36mModel.fit\u001B[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[0;32m   1402\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m tf\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mexperimental\u001B[38;5;241m.\u001B[39mTrace(\n\u001B[0;32m   1403\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m   1404\u001B[0m     epoch_num\u001B[38;5;241m=\u001B[39mepoch,\n\u001B[0;32m   1405\u001B[0m     step_num\u001B[38;5;241m=\u001B[39mstep,\n\u001B[0;32m   1406\u001B[0m     batch_size\u001B[38;5;241m=\u001B[39mbatch_size,\n\u001B[0;32m   1407\u001B[0m     _r\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m):\n\u001B[0;32m   1408\u001B[0m   callbacks\u001B[38;5;241m.\u001B[39mon_train_batch_begin(step)\n\u001B[1;32m-> 1409\u001B[0m   tmp_logs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1410\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m data_handler\u001B[38;5;241m.\u001B[39mshould_sync:\n\u001B[0;32m   1411\u001B[0m     context\u001B[38;5;241m.\u001B[39masync_wait()\n",
      "File \u001B[1;32m~\\.conda\\envs\\DAA\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    148\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    149\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 150\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    151\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    152\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[1;32m~\\.conda\\envs\\DAA\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001B[0m, in \u001B[0;36mFunction.__call__\u001B[1;34m(self, *args, **kwds)\u001B[0m\n\u001B[0;32m    912\u001B[0m compiler \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mxla\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jit_compile \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnonXla\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    914\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m OptionalXlaContext(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jit_compile):\n\u001B[1;32m--> 915\u001B[0m   result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n\u001B[0;32m    917\u001B[0m new_tracing_count \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexperimental_get_tracing_count()\n\u001B[0;32m    918\u001B[0m without_tracing \u001B[38;5;241m=\u001B[39m (tracing_count \u001B[38;5;241m==\u001B[39m new_tracing_count)\n",
      "File \u001B[1;32m~\\.conda\\envs\\DAA\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001B[0m, in \u001B[0;36mFunction._call\u001B[1;34m(self, *args, **kwds)\u001B[0m\n\u001B[0;32m    944\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock\u001B[38;5;241m.\u001B[39mrelease()\n\u001B[0;32m    945\u001B[0m   \u001B[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001B[39;00m\n\u001B[0;32m    946\u001B[0m   \u001B[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001B[39;00m\n\u001B[1;32m--> 947\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_stateless_fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)  \u001B[38;5;66;03m# pylint: disable=not-callable\u001B[39;00m\n\u001B[0;32m    948\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_stateful_fn \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    949\u001B[0m   \u001B[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001B[39;00m\n\u001B[0;32m    950\u001B[0m   \u001B[38;5;66;03m# in parallel.\u001B[39;00m\n\u001B[0;32m    951\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock\u001B[38;5;241m.\u001B[39mrelease()\n",
      "File \u001B[1;32m~\\.conda\\envs\\DAA\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2453\u001B[0m, in \u001B[0;36mFunction.__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   2450\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock:\n\u001B[0;32m   2451\u001B[0m   (graph_function,\n\u001B[0;32m   2452\u001B[0m    filtered_flat_args) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_maybe_define_function(args, kwargs)\n\u001B[1;32m-> 2453\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mgraph_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_flat\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   2454\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfiltered_flat_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcaptured_inputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgraph_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcaptured_inputs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\.conda\\envs\\DAA\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1860\u001B[0m, in \u001B[0;36mConcreteFunction._call_flat\u001B[1;34m(self, args, captured_inputs, cancellation_manager)\u001B[0m\n\u001B[0;32m   1856\u001B[0m possible_gradient_type \u001B[38;5;241m=\u001B[39m gradients_util\u001B[38;5;241m.\u001B[39mPossibleTapeGradientTypes(args)\n\u001B[0;32m   1857\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (possible_gradient_type \u001B[38;5;241m==\u001B[39m gradients_util\u001B[38;5;241m.\u001B[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001B[0;32m   1858\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m executing_eagerly):\n\u001B[0;32m   1859\u001B[0m   \u001B[38;5;66;03m# No tape is watching; skip to running the function.\u001B[39;00m\n\u001B[1;32m-> 1860\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_call_outputs(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_inference_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcall\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1861\u001B[0m \u001B[43m      \u001B[49m\u001B[43mctx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcancellation_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcancellation_manager\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m   1862\u001B[0m forward_backward \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_select_forward_and_backward_functions(\n\u001B[0;32m   1863\u001B[0m     args,\n\u001B[0;32m   1864\u001B[0m     possible_gradient_type,\n\u001B[0;32m   1865\u001B[0m     executing_eagerly)\n\u001B[0;32m   1866\u001B[0m forward_function, args_with_tangents \u001B[38;5;241m=\u001B[39m forward_backward\u001B[38;5;241m.\u001B[39mforward()\n",
      "File \u001B[1;32m~\\.conda\\envs\\DAA\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:497\u001B[0m, in \u001B[0;36m_EagerDefinedFunction.call\u001B[1;34m(self, ctx, args, cancellation_manager)\u001B[0m\n\u001B[0;32m    495\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m _InterpolateFunctionError(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    496\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m cancellation_manager \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 497\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[43mexecute\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexecute\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    498\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msignature\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    499\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnum_outputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_num_outputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    500\u001B[0m \u001B[43m        \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    501\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattrs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattrs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    502\u001B[0m \u001B[43m        \u001B[49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mctx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    503\u001B[0m   \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    504\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m execute\u001B[38;5;241m.\u001B[39mexecute_with_cancellation(\n\u001B[0;32m    505\u001B[0m         \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msignature\u001B[38;5;241m.\u001B[39mname),\n\u001B[0;32m    506\u001B[0m         num_outputs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_outputs,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    509\u001B[0m         ctx\u001B[38;5;241m=\u001B[39mctx,\n\u001B[0;32m    510\u001B[0m         cancellation_manager\u001B[38;5;241m=\u001B[39mcancellation_manager)\n",
      "File \u001B[1;32m~\\.conda\\envs\\DAA\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001B[0m, in \u001B[0;36mquick_execute\u001B[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     53\u001B[0m   ctx\u001B[38;5;241m.\u001B[39mensure_initialized()\n\u001B[1;32m---> 54\u001B[0m   tensors \u001B[38;5;241m=\u001B[39m \u001B[43mpywrap_tfe\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTFE_Py_Execute\u001B[49m\u001B[43m(\u001B[49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_handle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mop_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     55\u001B[0m \u001B[43m                                      \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattrs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_outputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     56\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m core\u001B[38;5;241m.\u001B[39m_NotOkStatusException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m     57\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "tuner.search(X, y, epochs=500, validation_split=0.2, callbacks=[stop_early])\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(f\"\"\"\n",
    "The hyperparameter search is complete. The optimal number of units in the first densely-connected\n",
    "layer is {best_hps.get('units')} and the optimal learning rate for the optimizer\n",
    "is {best_hps.get('learning_rate')}.\n",
    "\"\"\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Build the model with the optimal hyperparameters and train it on the data for 50 epochs\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "history = model.fit(X, y, epochs=50, validation_split=0.2)\n",
    "\n",
    "val_acc_per_epoch = history.history['val_accuracy']\n",
    "best_epoch = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1\n",
    "print('Best epoch: %d' % (best_epoch,))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "hypermodel = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "# Retrain the model\n",
    "hypermodel.fit(X, y, epochs=best_epoch, validation_split=0.2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "eval_result = hypermodel.evaluate(X, y)\n",
    "print(\"[test loss, test accuracy]:\", eval_result)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "-------\n",
    "-------\n",
    "-------"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=150, batch_size=32)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "\n",
    "acc = history_dict['accuracy']\n",
    "loss = history_dict['loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# \"bo\" is for \"blue dot\"\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "# b is for \"solid blue line\"\n",
    "plt.plot(epochs, acc, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predicts = model.predict(X_test)\n",
    "\n",
    "categories_predicted = [np.argmax(pred) for pred in predicts]\n",
    "\n",
    "categories_predicted"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Obter as previsões no dataset de submissão"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X = neural_network_data_preparation(test_df)\n",
    "\n",
    "scaler_X = MinMaxScaler(feature_range=(0, 1)).fit(X)\n",
    "X_scaled = pd.DataFrame(scaler_X.transform(X[X.columns]), columns=X.columns)\n",
    "\n",
    "X_scaled"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "categories_prob_predictions = model.predict(X_scaled)\n",
    "\n",
    "categories_prob_predictions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "numerical_predictions = [np.argmax(pred) for pred in categories_prob_predictions]\n",
    "\n",
    "numerical_predictions[:10]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "numerical_predictions_df = pd.DataFrame(numerical_predictions)\n",
    "\n",
    "incidents_categories = {\n",
    "    0: 'None',\n",
    "    1: 'Low',\n",
    "    2: 'Medium',\n",
    "    3: 'High',\n",
    "    4: 'Very_High',\n",
    "}\n",
    "\n",
    "predictions_df = numerical_predictions_df.replace(incidents_categories)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predictions_df.index += 1\n",
    "\n",
    "predictions_df.to_csv(\"submission.csv\", header=['Incidents'], index_label='RowId')"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

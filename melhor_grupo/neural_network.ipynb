{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b856b60-22f2-493e-a91d-83c3b4bc68d7",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5f0dbd9-a10f-4a55-9b16-d6ff09fae607",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from pandas import DatetimeIndex\n",
    "\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2edeaf-98b3-452e-a88b-8daa1d68f226",
   "metadata": {},
   "source": [
    "### Carregamento dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20203651-46ed-4cf0-bdd3-20d4e6c8ee18",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataOriginal = pd.read_csv('new_Global_Superstore2.csv', index_col=0, comment='#') \n",
    "df = dataOriginal "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790805ff-ab09-4658-b9f5-f5a04542bc49",
   "metadata": {},
   "source": [
    "### Tratamento dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8a318407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.56909239 -0.86822074 -3.30504661 ... -0.59934891  1.22454555\n",
      "  -0.31475317]\n",
      " [-0.38104867  0.43683774  0.12487042 ... -0.59934891  1.22454555\n",
      "  -1.41402756]\n",
      " [-1.47507053 -0.86822074  0.12487042 ... -0.59934891  1.22454555\n",
      "  -1.41402756]\n",
      " ...\n",
      " [ 0.71297319 -0.86822074  0.12487042 ...  3.04980699 -0.81662948\n",
      "  -0.31475317]\n",
      " [ 0.71297319  1.74189622  0.12487042 ... -0.59934891 -0.81662948\n",
      "   1.33415841]\n",
      " [-0.38104867 -0.86822074  0.12487042 ... -0.59934891  1.22454555\n",
      "   1.33415841]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "'''\n",
    "Links:\n",
    "https://stackoverflow.com/questions/44552031/sklearnstandardscaler-can-i-inverse-the-standardscaler-for-the-model-output\n",
    "https://stackoverflow.com/questions/56936780/how-do-i-correctly-use-the-inverse-transform-method-when-using-powertransformer\n",
    "'''\n",
    "\n",
    "\n",
    "df = dataOriginal\n",
    "cols = df.columns\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df[cols])\n",
    "\n",
    "df = scaler.transform(df[cols])\n",
    "\n",
    "'''\n",
    "pt = PowerTransformer()\n",
    "pt.fit(df)\n",
    "\n",
    "df = pt.transform(df)\n",
    "scaler = MinMaxScaler(feature_range = (0,1))\n",
    "df = scaler.fit_transform(df)\n",
    "'''\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e383f3a1-2934-4ebe-bc11-fb9c0f1fe0be",
   "metadata": {},
   "source": [
    "### Neurónios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "097cb478-22be-4c57-8dd2-ee4117930d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_MLP = dataset.drop(columns=['Order ID', 'Customer ID', 'Customer Name', 'Product ID', 'Product Name', 'Postal Code'])\n",
    "dataset_MLP = df\n",
    "\n",
    "#dataset_MLP = dataset_MLP.drop(['Order Priority', 'Market', \"City\", \"Segment\", \"Ship Mode\", \"Ship Date\"], axis=1)\n",
    "#print(dataset_MLP.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "61a945d2-3161-4a80-aa10-251be0d9fbc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: 2.10.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, KFold, train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "RANDOM_SEED = 2021\n",
    "\n",
    "print(\"Tensorflow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "44b207a7-b669-48dd-aa67-ce269d8dd30f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'drop'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m X_MLP \u001b[38;5;241m=\u001b[39m \u001b[43mdataset_MLP\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProfit\u001b[39m\u001b[38;5;124m'\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      2\u001b[0m y_MLP \u001b[38;5;241m=\u001b[39m dataset_MLP[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProfit\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[0;32m      4\u001b[0m small_dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'drop'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "X_MLP = dataset_MLP.drop('Profit', axis=1)\n",
    "y_MLP = dataset_MLP[['Profit']]\n",
    "\n",
    "small_dataset = True\n",
    "if small_dataset: \n",
    "    remove_n = 20000\n",
    "    drop_indices = np.random.choice(dataset_MLP.index, remove_n, replace=False)\n",
    "    dataset_MLP_subset = dataset_MLP.drop(drop_indices)\n",
    "    X_MLP = dataset_MLP_subset.drop('Profit', axis=1)\n",
    "    y_MLP = dataset_MLP_subset[['Profit']]\n",
    "\n",
    " \n",
    "    \n",
    "    # Convert data\n",
    "if \"Order Date\" in X_MLP:\n",
    "    consider_year = True\n",
    "    consider_month = True\n",
    "    consider_day = True\n",
    "else: \n",
    "    consider_year = False\n",
    "    consider_month = False\n",
    "    consider_day = False\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec623efb-eac3-4f70-85d3-93a9a428f470",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Onde fui buscar algumas coisas:\n",
    "# https://www.projectpro.io/recipes/find-optimal-parameters-using-gridsearchcv\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_MLP, y_MLP, test_size=0.2, random_state=RANDOM_SEED)\n",
    "\n",
    "number_columns = len(X_scaled.columns)#.count()\n",
    "#print(\"Número de colunas: \", number_columns)\n",
    "\n",
    "def build_model(activation='relu', learning_rate=0.01):\n",
    "#create a sequential model (with three Layers - Last one is the output)\n",
    "    model = Sequential()\n",
    "    model.add(Dense(number_columns, input_dim=number_columns, activation=activation))\n",
    "    model.add(Dense(6, activation=activation))\n",
    "    model.add(Dense(1, activation='relu'))\n",
    "  \n",
    "     #compile the model\n",
    "    #Define the Loss function, the otimizer and metrics to be used\n",
    "    model. compile(\n",
    "        loss = 'mae',\n",
    "        optimizer = tf.optimizers.Adam(learning_rate),\n",
    "        metrics = ['mae', 'mse'])\n",
    "    return model\n",
    "\n",
    "model = build_model()\n",
    "model.summary()\n",
    "''' Este é melhor, mas mais lento\n",
    "TUNING_DICT = {\n",
    "    'activation' : ['relu', 'sigmoid'],\n",
    "    'learning_rate' : [0.01, 0.001]\n",
    "}\n",
    "'''\n",
    "# = {\n",
    "#    'activation' : ['relu'],\n",
    "#    'learning_rate' : [0.01]\n",
    "#}\n",
    "\n",
    "TUNING_DICT = {'learning_rate': [0.01,0.03, 0.1],                  \n",
    "                'activation' : ['relu', 'sigmoid'],\n",
    "                 }\n",
    "\n",
    "kf = KFold(n_splits=2, shuffle=True, random_state=RANDOM_SEED)\n",
    "\n",
    "model = KerasRegressor(build_fn=build_model, epochs=15,batch_size=number_columns)\n",
    "grid_search = GridSearchCV(estimator = model,\n",
    "                            param_grid = TUNING_DICT,\n",
    "                            cv = kf,\n",
    "                            scoring = 'neg_mean_absolute_error',\n",
    "                            refit ='True',\n",
    "                            verbose = 1)\n",
    "\n",
    "grid_search.fit(X_train, y_train, validation_split=0.2)\n",
    "\n",
    "print(\" Results from Grid Search \" )\n",
    "print(grid_search)\n",
    "#print(\"\\n The best score across ALL searched params:\\n\",grid_search.best_score_)\n",
    "#print(\"\\n The best parameters across ALL searched params:\\n\",grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88aa5f0-3750-4740-9b15-346a15a56f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overfitting analisys\n",
    "\n",
    "#Our best model (remember we set refit\n",
    "best_mlp_model = grid_search.best_estimator_\n",
    "\n",
    "from livelossplot import PlotLossesKerasTF\n",
    "best_mlp_model.fit(X_train, y_train, epochs=20,\n",
    "validation_data=(X_test, y_test),\n",
    "callbacks=[PlotLossesKerasTF()], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df0dca7",
   "metadata": {},
   "source": [
    "### Análise de overfitting\n",
    "\n",
    "Pelos gráficos, vemos que o medium average error (mae) dá bastante mal, mas o mse dá bem. Como 20 epochs é um valor considerável, consideramos nesta fase melhorar o tratamento de dados, e depois verificar se há melhorias. \n",
    "Isto porque, o tratamento de dados atual para este modelo é quase com o objetivo de o modelo consiguir analisar o dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25ad198-237d-4c9b-967f-01e32218b85f",
   "metadata": {},
   "outputs": [],
   "source": [
    " #0btain predictions\n",
    "\n",
    "predictions = best_mlp_model.predict(X_test)\n",
    "\n",
    "\n",
    "predictions = predictions.reshape(predictions.shape[0], 1)\n",
    "predicions_unscaled = scaler_y.inverse_transform(predictions)\n",
    "predicions_unscaled[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d744c38e-6c44-45c4-b1e5-106c72871f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_unscaled = scaler_y.inverse_transform(y_test)\n",
    "\n",
    "#Visualising the actual and predicted result\n",
    "def real_predicted_viz(limit):\n",
    "    plt.figure(figsize=(14,6))\n",
    "    plt.plot(y_test_unscaled[:limit], color = 'green', label = 'Actual')\n",
    "    plt.plot (predicions_unscaled[:limit], color = 'red', label = 'Predicted')\n",
    "    plt.grid(alpha = 0.3)\n",
    "    plt.xlabel('Sample')\n",
    "    plt.ylabel ('Profit')\n",
    "    plt.title('Real vs Predicted')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "#let's Limit to 200 comparisions for better visualization\n",
    "real_predicted_viz(400)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd9685b",
   "metadata": {},
   "source": [
    "Pela análise do gráfico, podemos ver que há algumas previsões aproximadamente corretas, mas existem algumas, bem visíveis, que mostram quão errado o nosso modelo está.\n",
    "Como dito anteriormente, falta tratamento de dados. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced79ae6-6b6c-45ab-be50-611519fbc0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Best: %f using %s\" % (grid_search.best_score_, grid_search.best_params_))\n",
    "means = grid_search.cv_results_['mean_test_score']\n",
    "stds = grid_search.cv_results_['std_test_score']\n",
    "params = grid_search.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"Média: %f Desvio padrão: (%f) com: %r\" % (mean, stdev, param))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8407e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.keras.losses.mean_absolute_error(y_test_unscaled, predicions_unscaled)\n",
    "mae = tf.keras.losses.MeanAbsoluteError()\n",
    "print(\"MAE: \", mae(y_test_unscaled, predicions_unscaled).numpy())\n",
    "#mse = tf.keras.losses.mean_squared_error(y_test_unscaled, predicions_unscaled).numpy()\n",
    "mse = np.mean(np.square(y_test_unscaled - predicions_unscaled))\n",
    "print(\"MSE: \", mse)\n",
    "print(\"RMSE: \", math.sqrt(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1428dcb4",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b856b60-22f2-493e-a91d-83c3b4bc68d7",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5f0dbd9-a10f-4a55-9b16-d6ff09fae607",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from pandas import DatetimeIndex\n",
    "\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2edeaf-98b3-452e-a88b-8daa1d68f226",
   "metadata": {},
   "source": [
    "### Carregamento dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20203651-46ed-4cf0-bdd3-20d4e6c8ee18",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataOriginal = pd.read_excel('Global_Superstore2.xlsx', index_col=0, comment='#') \n",
    "df = dataOriginal "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790805ff-ab09-4658-b9f5-f5a04542bc49",
   "metadata": {},
   "source": [
    "### Tratamento dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05895e64-b95c-4dc1-9a4f-88c0c178ced9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mudar o tipo das colunas\n",
    "df['Ship Mode'] = df['Ship Mode'].astype('category')\n",
    "df['Segment'] = df['Segment'].astype('category')\n",
    "df['City'] = df['Country'].astype('category')\n",
    "df['State'] = df['Country'].astype('category')\n",
    "df['Country'] = df['Country'].astype('category')\n",
    "df['Market'] = df['Market'].astype('category')\n",
    "df['Region'] = df['Region'].astype('category')\n",
    "df['Category'] = df['Category'].astype('category')\n",
    "df['Sub-Category'] = df['Sub-Category'].astype('category')\n",
    "#df['Orde Priority'] = df['Order Priority'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50d5ec20-1efb-49d6-b7f2-e97c21d32fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tirar valores nulos\n",
    "if df['Profit'].isnull().sum() > 0:\n",
    "    df = df[df['Profit'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "864a5881-bfa7-4904-9eca-5b058354e746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de valores nulos \n",
      "Order ID          0\n",
      "Order Date        0\n",
      "Ship Date         0\n",
      "Ship Mode         0\n",
      "Customer ID       0\n",
      "Customer Name     0\n",
      "Segment           0\n",
      "City              0\n",
      "State             0\n",
      "Country           0\n",
      "Market            0\n",
      "Region            0\n",
      "Product ID        0\n",
      "Category          0\n",
      "Sub-Category      0\n",
      "Product Name      0\n",
      "Sales             0\n",
      "Quantity          0\n",
      "Discount          0\n",
      "Profit            0\n",
      "Shipping_Cost     0\n",
      "Order_Priority    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Já está justificado no ficheiro data_exploration o porquê de removermos Códigos postais.\n",
    "if 'Postal Code' in df.columns:\n",
    "    df = df.drop('Postal Code', axis=1)\n",
    " \n",
    "# Mudar nome\n",
    "df = df.rename(columns = {'Shipping Cost':'Shipping_Cost'})\n",
    "df = df.rename(columns = {'Order Priority':'Order_Priority'})\n",
    "\n",
    "#data['Product Name'] = data['Product Name'].mode().iloc[0]\n",
    "#data.Order_Priority = most_imputer2.fit_transform(data[['Order_Priority']])\n",
    "\n",
    "print(\"Total de valores nulos \")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6247bd9f-90e6-4b22-b8c0-6f0a4a1420ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " City 's com mais de 1% de valores\n",
      "\n",
      "Other                 24.707031\n",
      "United States         19.343750\n",
      "Australia              5.541016\n",
      "France                 5.521484\n",
      "Mexico                 5.164062\n",
      "Germany                4.033203\n",
      "China                  3.671875\n",
      "United Kingdom         3.189453\n",
      "Brazil                 3.123047\n",
      "India                  3.037109\n",
      "Indonesia              2.714844\n",
      "Turkey                 2.691406\n",
      "Italy                  2.164062\n",
      "Nigeria                1.767578\n",
      "Spain                  1.677734\n",
      "Dominican Republic     1.449219\n",
      "El Salvador            1.437500\n",
      "Cuba                   1.414062\n",
      "Honduras               1.392578\n",
      "Philippines            1.330078\n",
      "New Zealand            1.226562\n",
      "Nicaragua              1.199219\n",
      "Iran                   1.185547\n",
      "Guatemala              1.017578\n",
      "Name: City, dtype: float64\n",
      "\n",
      " State 's com mais de 1% de valores\n",
      "\n",
      "Other                 24.707031\n",
      "United States         19.343750\n",
      "Australia              5.541016\n",
      "France                 5.521484\n",
      "Mexico                 5.164062\n",
      "Germany                4.033203\n",
      "China                  3.671875\n",
      "United Kingdom         3.189453\n",
      "Brazil                 3.123047\n",
      "India                  3.037109\n",
      "Indonesia              2.714844\n",
      "Turkey                 2.691406\n",
      "Italy                  2.164062\n",
      "Nigeria                1.767578\n",
      "Spain                  1.677734\n",
      "Dominican Republic     1.449219\n",
      "El Salvador            1.437500\n",
      "Cuba                   1.414062\n",
      "Honduras               1.392578\n",
      "Philippines            1.330078\n",
      "New Zealand            1.226562\n",
      "Nicaragua              1.199219\n",
      "Iran                   1.185547\n",
      "Guatemala              1.017578\n",
      "Name: State, dtype: float64\n",
      "\n",
      " Country 's com mais de 1% de valores\n",
      "\n",
      "Other                 24.707031\n",
      "United States         19.343750\n",
      "Australia              5.541016\n",
      "France                 5.521484\n",
      "Mexico                 5.164062\n",
      "Germany                4.033203\n",
      "China                  3.671875\n",
      "United Kingdom         3.189453\n",
      "Brazil                 3.123047\n",
      "India                  3.037109\n",
      "Indonesia              2.714844\n",
      "Turkey                 2.691406\n",
      "Italy                  2.164062\n",
      "Nigeria                1.767578\n",
      "Spain                  1.677734\n",
      "Dominican Republic     1.449219\n",
      "El Salvador            1.437500\n",
      "Cuba                   1.414062\n",
      "Honduras               1.392578\n",
      "Philippines            1.330078\n",
      "New Zealand            1.226562\n",
      "Nicaragua              1.199219\n",
      "Iran                   1.185547\n",
      "Guatemala              1.017578\n",
      "Name: Country, dtype: float64\n",
      "\n",
      " Region 's com mais de 1% de valores\n",
      "\n",
      "Central           21.683594\n",
      "South             12.947266\n",
      "EMEA               9.822266\n",
      "North              9.345703\n",
      "Africa             8.958984\n",
      "Oceania            6.810547\n",
      "West               6.191406\n",
      "Southeast Asia     6.111328\n",
      "East               5.511719\n",
      "North Asia         4.566406\n",
      "Central Asia       4.000000\n",
      "Caribbean          3.300781\n",
      "Name: Region, dtype: float64\n",
      "\n",
      " Sub-Category 's com mais de 1% de valores\n",
      "\n",
      "Binders        12.015625\n",
      "Storage         9.880859\n",
      "Art             9.464844\n",
      "Paper           6.910156\n",
      "Chairs          6.707031\n",
      "Phones          6.556641\n",
      "Furnishings     6.191406\n",
      "Accessories     6.005859\n",
      "Labels          5.089844\n",
      "Supplies        4.736328\n",
      "Fasteners       4.726562\n",
      "Bookcases       4.708984\n",
      "Envelopes       4.652344\n",
      "Copiers         4.341797\n",
      "Appliances      3.427734\n",
      "Machines        2.902344\n",
      "Tables          1.681641\n",
      "Name: Sub-Category, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Juntar as entradas com pouca representatividade no dataset, para eliminar overfitting.\n",
    "\n",
    "def show_frequencies_categorys():\n",
    "    for coluna in df.select_dtypes(exclude=[\"number\",\"bool_\", \"float64\"]).columns:\n",
    "        #print(coluna)\n",
    "        if df[coluna].nunique() > 10 and (\"Date\" not in coluna) and (\"ID\" not in coluna):\n",
    "\n",
    "            \n",
    "            #df2=dfgroupby([coluna])[coluna].sum().rename(\"Courses_fee\").groupby(level = 0).transform(lambda x: x/x.sum())\n",
    "            coluna_percentagens = (df[coluna].value_counts()/df[coluna].count())*100\n",
    "           # print(coluna_percentagens)\n",
    "           # print(\"Exp:\")\n",
    "            values = coluna_percentagens.groupby(coluna_percentagens > 1).filter(lambda x: x.mean() > 1)\n",
    "            if values.size > 0:\n",
    "                print(\"\\n\", coluna, \"'s com mais de 1% de valores\\n\")\n",
    "                print(values)\n",
    "\n",
    "\n",
    "# Juntar países e cidades cuja frequência é menor que uma especificada, para dados categóricos\n",
    "# Esses dados são: \"State\", \"City\", \"Country\", \"Market\", \"Region\"\n",
    "percentage = 1\n",
    "\n",
    "dataset = df\n",
    "for coluna in [\"State\", \"City\", \"Country\", \"Market\", \"Region\" ]:\n",
    "\n",
    "        #print(coluna)\n",
    "        series = dataset.value_counts(dataset[coluna])\n",
    "        mask = (series/series.sum() * 100).lt(percentage)\n",
    "        #print(mask,\"\\n\\n\")\n",
    "        dataset[coluna] = np.where(dataset[coluna].isin(series[mask].index),'Other',dataset[coluna])\n",
    "\n",
    "df = dataset\n",
    "\n",
    "show_frequencies_categorys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5532b3e2-a0d3-4c1d-a014-71d627738184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Order ID  Order Date   Ship Date       Ship Mode Customer ID  \\\n",
      "Row ID                                                                        \n",
      "32298    CA-2012-124891  31-07-2012  31-07-2012        Same Day    RH-19495   \n",
      "26341     IN-2013-77878  05-02-2013  07-02-2013    Second Class    JR-16210   \n",
      "25330     IN-2013-71249  17-10-2013  18-10-2013     First Class    CR-12730   \n",
      "13524   ES-2013-1579342  28-01-2013  30-01-2013     First Class    KM-16375   \n",
      "47221      SG-2013-4320  05-11-2013  06-11-2013        Same Day     RH-9495   \n",
      "...                 ...         ...         ...             ...         ...   \n",
      "24175     IN-2014-57662  05-08-2014  10-08-2014  Standard Class    DB-13270   \n",
      "29002     IN-2014-62366  19-06-2014  19-06-2014        Same Day    KE-16420   \n",
      "35398    US-2014-102288  20-06-2014  24-06-2014  Standard Class    ZC-21910   \n",
      "9596     MX-2012-140767  18-02-2012  22-02-2012  Standard Class    RB-19795   \n",
      "6147     MX-2012-134460  22-05-2012  26-05-2012    Second Class    MC-18100   \n",
      "\n",
      "            Customer Name      Segment           City          State  \\\n",
      "Row ID                                                                 \n",
      "32298         Rick Hansen     Consumer  United States  United States   \n",
      "26341       Justin Ritter    Corporate      Australia      Australia   \n",
      "25330        Craig Reiter     Consumer      Australia      Australia   \n",
      "13524    Katherine Murray  Home Office        Germany        Germany   \n",
      "47221         Rick Hansen     Consumer          Other          Other   \n",
      "...                   ...          ...            ...            ...   \n",
      "24175   Deborah Brumfield  Home Office      Australia      Australia   \n",
      "29002     Katrina Edelman    Corporate          Other          Other   \n",
      "35398    Zuschuss Carroll     Consumer  United States  United States   \n",
      "9596           Ross Baird  Home Office         Brazil         Brazil   \n",
      "6147        Mick Crebagga     Consumer      Nicaragua      Nicaragua   \n",
      "\n",
      "              Country  ...        Product ID         Category Sub-Category  \\\n",
      "Row ID                 ...                                                   \n",
      "32298   United States  ...   TEC-AC-10003033       Technology  Accessories   \n",
      "26341       Australia  ...   FUR-CH-10003950        Furniture       Chairs   \n",
      "25330       Australia  ...   TEC-PH-10004664       Technology       Phones   \n",
      "13524         Germany  ...   TEC-PH-10004583       Technology       Phones   \n",
      "47221           Other  ...  TEC-SHA-10000501       Technology      Copiers   \n",
      "...               ...  ...               ...              ...          ...   \n",
      "24175       Australia  ...   OFF-BI-10002424  Office Supplies      Binders   \n",
      "29002           Other  ...   OFF-FA-10000746  Office Supplies    Fasteners   \n",
      "35398   United States  ...   OFF-AP-10002906  Office Supplies   Appliances   \n",
      "9596           Brazil  ...   OFF-BI-10000806  Office Supplies      Binders   \n",
      "6147        Nicaragua  ...   OFF-PA-10004155  Office Supplies        Paper   \n",
      "\n",
      "                                             Product Name     Sales Quantity  \\\n",
      "Row ID                                                                         \n",
      "32298   Plantronics CS510 - Over-the-Head monaural Wir...  2309.650      7.0   \n",
      "26341           Novimex Executive Leather Armchair, Black  3709.395      9.0   \n",
      "25330                   Nokia Smart Phone, with Caller ID  5175.171      9.0   \n",
      "13524                      Motorola Smart Phone, Cordless  2892.510      5.0   \n",
      "47221                      Sharp Wireless Fax, High-Speed  2832.960      8.0   \n",
      "...                                                   ...       ...      ...   \n",
      "24175                               Avery Binder, Economy    58.050      5.0   \n",
      "29002                       Advantus Thumb Tacks, 12 Pack    65.100      5.0   \n",
      "35398   Hoover Replacement Belt for Commercial Guardsm...     0.444      1.0   \n",
      "9596                              Acco Index Tab, Economy    13.440      2.0   \n",
      "6147              Eaton Computer Printout Paper, 8.5 x 11    61.380      3.0   \n",
      "\n",
      "         Discount    Profit Shipping_Cost  Order_Priority  \n",
      "Row ID                                                     \n",
      "32298   0.00-0.17  762.1845        933.57        Critical  \n",
      "26341   0.00-0.17 -288.7650        923.63        Critical  \n",
      "25330   0.00-0.17  919.9710        915.49          Medium  \n",
      "13524   0.00-0.17  -96.5400        910.16          Medium  \n",
      "47221   0.00-0.17  311.5200        903.04        Critical  \n",
      "...           ...       ...           ...             ...  \n",
      "24175   0.00-0.17   19.9500          0.01          Medium  \n",
      "29002   0.00-0.17    4.5000          0.01          Medium  \n",
      "35398   0.68-0.85   -1.1100          0.01          Medium  \n",
      "9596    0.00-0.17    2.4000          0.00          Medium  \n",
      "6147    0.00-0.17    1.8000          0.00            High  \n",
      "\n",
      "[51200 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "# Meter os valores de discount em bins\n",
    "\n",
    "data = df\n",
    "number_bins = 6\n",
    "\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "\n",
    "if is_numeric_dtype(data['Discount']):\n",
    "    # Bins in discount\n",
    "   \n",
    "    min_value = data['Discount'].min()\n",
    "    \n",
    "    max_value = data['Discount'].max()\n",
    "    limits_bins = np.linspace(min_value,max_value , num=number_bins)\n",
    "    labels_names = []\n",
    "    for i in range(0, len(limits_bins) - 1):\n",
    "        v1 = '{:.2f}'.format(limits_bins[i])\n",
    "        v2 = '{:.2f}'.format(limits_bins[i+1])\n",
    "        labels_names.append(f'{v1}-{v2}')\n",
    "    #print(\"v1: \", limits_bins[i])\n",
    "    #print(\"v2: \", limits_bins[i+1])\n",
    "    #print(\"----\")\n",
    "#    data4.loc[data4['score'].between(0, 50, 'both'), 'grade'] = 'C'\n",
    "#print(labels_names)\n",
    "    data['Discount'] = pd.cut(x = data['Discount'], bins = limits_bins, labels = labels_names, include_lowest = True)\n",
    "    #print(data)\n",
    "df = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d579766-e8f4-4699-b3fd-069fab896bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tratar das datas\n",
    "data2 = df\n",
    "\n",
    "if \"Order Date\" in data2:\n",
    "    consider_year = True\n",
    "    consider_month = True\n",
    "    consider_day = True\n",
    "else: \n",
    "    consider_year = False\n",
    "    consider_month = False\n",
    "    consider_day = False\n",
    "    \n",
    "# Converter datas, acho que já está feito em cima, depois confirmar\n",
    "dates_order = DatetimeIndex(pd.to_datetime(data2['Order Date'],format='%d-%m-%Y', errors='coerce'))\n",
    "dates_ship = DatetimeIndex(pd.to_datetime(data2['Ship Date'],format='%d-%m-%Y', errors='coerce'))\n",
    "if consider_year : \n",
    "    data2['Year_order'] = dates_order.year\n",
    "    data2['Year_ship'] = dates_ship.year\n",
    "    \n",
    "if consider_month : \n",
    "    data2['Month_order'] = dates_order.month\n",
    "    data2['Month_ship'] = dates_ship.month\n",
    "\n",
    "if consider_day : \n",
    "    data2['Day_order'] = dates_order.day\n",
    "    data2['Day_ship'] = dates_ship.day\n",
    "if 'Order Date' in data2:\n",
    "    data2 = data2.drop(['Order Date', 'Ship Date'], axis=1)\n",
    "\n",
    "df = data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a4e133e-7973-4bc3-84d1-6c1aa781e21a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Após fazer Label Encoding\n",
      "        Order ID  Ship Mode  Customer ID  Customer Name  Segment  City  State  \\\n",
      "Row ID                                                                          \n",
      "32298       1493          1         1286            632        0    23     23   \n",
      "26341      13042          2          808            413        1     0      0   \n",
      "25330      12962          0          336            181        0     0      0   \n",
      "13524       6792          0          873            424        2     7      7   \n",
      "47221      21681          1         1290            632        0    18     18   \n",
      "...          ...        ...          ...            ...      ...   ...    ...   \n",
      "24175      13967          3          383            217        2     0      0   \n",
      "29002      14047          1          854            427        1    18     18   \n",
      "35398      24213          3         1587            793        0    23     23   \n",
      "9596       17682          3         1249            652        2     1      1   \n",
      "6147       17618          2          981            539        0    16     16   \n",
      "\n",
      "        Country  Market  Region  ...  Discount    Profit  Shipping_Cost  \\\n",
      "Row ID                           ...                                      \n",
      "32298        23       6       5  ...         0  762.1845         933.57   \n",
      "26341         0       0       8  ...         0 -288.7650         923.63   \n",
      "25330         0       0       8  ...         0  919.9710         915.49   \n",
      "13524         7       3       2  ...         0  -96.5400         910.16   \n",
      "47221        18       1       0  ...         0  311.5200         903.04   \n",
      "...         ...     ...     ...  ...       ...       ...            ...   \n",
      "24175         0       0       8  ...         0   19.9500           0.01   \n",
      "29002        18       0       7  ...         0    4.5000           0.01   \n",
      "35398        23       6       2  ...         4   -1.1100           0.01   \n",
      "9596          1       4      10  ...         0    2.4000           0.00   \n",
      "6147         16       4       2  ...         0    1.8000           0.00   \n",
      "\n",
      "        Order_Priority  Year_order  Year_ship  Month_order  Month_ship  \\\n",
      "Row ID                                                                   \n",
      "32298                0        2012       2012            7           7   \n",
      "26341                0        2013       2013            2           2   \n",
      "25330                3        2013       2013           10          10   \n",
      "13524                3        2013       2013            1           1   \n",
      "47221                0        2013       2013           11          11   \n",
      "...                ...         ...        ...          ...         ...   \n",
      "24175                3        2014       2014            8           8   \n",
      "29002                3        2014       2014            6           6   \n",
      "35398                3        2014       2014            6           6   \n",
      "9596                 3        2012       2012            2           2   \n",
      "6147                 1        2012       2012            5           5   \n",
      "\n",
      "        Day_order  Day_ship  \n",
      "Row ID                       \n",
      "32298          31        31  \n",
      "26341           5         7  \n",
      "25330          17        18  \n",
      "13524          28        30  \n",
      "47221           5         6  \n",
      "...           ...       ...  \n",
      "24175           5        10  \n",
      "29002          19        19  \n",
      "35398          20        24  \n",
      "9596           18        22  \n",
      "6147           22        26  \n",
      "\n",
      "[51200 rows x 26 columns]\n"
     ]
    }
   ],
   "source": [
    "# Label encoding de algumas variáveis\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "lb_make = LabelEncoder()\n",
    "\n",
    "def do_LabelEncoding(dataset, col_name):\n",
    "    if col_name in dataset:\n",
    "        dataset[col_name] = lb_make.fit_transform(dataset[col_name])\n",
    "    return dataset   \n",
    "\n",
    "if \"Category\" in data2:\n",
    "    process_Product_ID = True\n",
    "    process_Costumer_ID = True\n",
    "    process_Category = True\n",
    "    process_Sub_Category = True\n",
    "    process_Discount = True\n",
    "else: \n",
    "    process_Product_ID = False\n",
    "    process_Costumer_ID = False\n",
    "    process_Category = False\n",
    "    process_Sub_Category = False\n",
    "    process_Discount = False\n",
    "\n",
    "for coluna in data2.select_dtypes(exclude=[\"number\",\"bool_\", \"float64\"]).columns:\n",
    "    data2 = do_LabelEncoding(data2, coluna)\n",
    "\n",
    "if data2['Ship Mode'].dtype == 'category' or data2['Ship Mode'].dtype == 'object':\n",
    "   # print(\"Ship mode ainda está em categorias\")\n",
    "    #print(data2['Ship Mode'].unique())\n",
    "    ship_mode_dic = {'Same Day':1, 'First Class':2, 'Second Class':3,'Standard Class':4}\n",
    "    #dataTemp = pd.DataFrame({\"Ship Mode\": ship_mode_dic.keys()})\n",
    "    #print(\"Aqui vai\\n\")\n",
    "    data2[\"Ship Mode\"]= data2[\"Ship Mode\"].apply(lambda x: ship_mode_dic.get(x))\n",
    "    #print(data2)\n",
    "\n",
    "if \"Order_Priority\" in data2 and (data2['Order_Priority'].dtype == 'category' or data2['Order_Priority'].dtype == 'object'):\n",
    "    dataTemp = data2\n",
    "    # print(\"Ship mode ainda está em categorias\")\n",
    "    #print(dataTemp['Order_Priority'].unique())\n",
    "    priorities_dic = {'Critical':1, 'High':2, 'Medium':3,'Low':4}\n",
    "    #dataTemp = pd.DataFrame({\"Order_Priority\": priorities_dic.keys()})\n",
    "    #print(\"Aqui vai\\n\")\n",
    "    dataTemp[\"Order_Priority\"]= dataTemp[\"Order_Priority\"].apply(lambda x: priorities_dic.get(x))\n",
    "    #print(dataTemp.info())\n",
    "    data2 = dataTemp\n",
    "\n",
    " \n",
    "\n",
    "print(\"\\nApós fazer Label Encoding\")\n",
    "print(data2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0567c17d",
   "metadata": {},
   "source": [
    "# Será que depois usamos estas coisas que metemos na análise geral?\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e383f3a1-2934-4ebe-bc11-fb9c0f1fe0be",
   "metadata": {},
   "source": [
    "### Neurónios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "097cb478-22be-4c57-8dd2-ee4117930d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 51200 entries, 32298 to 6147\n",
      "Data columns (total 26 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   Order ID        51200 non-null  int32  \n",
      " 1   Ship Mode       51200 non-null  int32  \n",
      " 2   Customer ID     51200 non-null  int32  \n",
      " 3   Customer Name   51200 non-null  int32  \n",
      " 4   Segment         51200 non-null  int32  \n",
      " 5   City            51200 non-null  int32  \n",
      " 6   State           51200 non-null  int32  \n",
      " 7   Country         51200 non-null  int32  \n",
      " 8   Market          51200 non-null  int32  \n",
      " 9   Region          51200 non-null  int32  \n",
      " 10  Product ID      51200 non-null  int32  \n",
      " 11  Category        51200 non-null  int32  \n",
      " 12  Sub-Category    51200 non-null  int32  \n",
      " 13  Product Name    51200 non-null  int32  \n",
      " 14  Sales           51200 non-null  float64\n",
      " 15  Quantity        51200 non-null  float64\n",
      " 16  Discount        51200 non-null  int32  \n",
      " 17  Profit          51200 non-null  float64\n",
      " 18  Shipping_Cost   51200 non-null  float64\n",
      " 19  Order_Priority  51200 non-null  int32  \n",
      " 20  Year_order      51200 non-null  int64  \n",
      " 21  Year_ship       51200 non-null  int64  \n",
      " 22  Month_order     51200 non-null  int64  \n",
      " 23  Month_ship      51200 non-null  int64  \n",
      " 24  Day_order       51200 non-null  int64  \n",
      " 25  Day_ship        51200 non-null  int64  \n",
      "dtypes: float64(4), int32(16), int64(6)\n",
      "memory usage: 7.4 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#dataset_MLP = dataset.drop(columns=['Order ID', 'Customer ID', 'Customer Name', 'Product ID', 'Product Name', 'Postal Code'])\n",
    "dataset_MLP = data2\n",
    "\n",
    "#dataset_MLP = dataset_MLP.drop(['Order Priority', 'Market', \"City\", \"Segment\", \"Ship Mode\", \"Ship Date\"], axis=1)\n",
    "print(dataset_MLP.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61a945d2-3161-4a80-aa10-251be0d9fbc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: 2.10.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, KFold, train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "RANDOM_SEED = 2021\n",
    "\n",
    "print(\"Tensorflow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b207a7-b669-48dd-aa67-ce269d8dd30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X_MLP = dataset_MLP.drop('Profit', axis=1)\n",
    "y_MLP = dataset_MLP[['Profit']]\n",
    "\n",
    "small_dataset = True\n",
    "if small_dataset: \n",
    "    remove_n = 20000\n",
    "    drop_indices = np.random.choice(dataset_MLP.index, remove_n, replace=False)\n",
    "    dataset_MLP_subset = dataset_MLP.drop(drop_indices)\n",
    "    X_MLP = dataset_MLP_subset.drop('Profit', axis=1)\n",
    "    y_MLP = dataset_MLP_subset[['Profit']]\n",
    "\n",
    " \n",
    "    \n",
    "    # Convert data\n",
    "if \"Order Date\" in X_MLP:\n",
    "    consider_year = True\n",
    "    consider_month = True\n",
    "    consider_day = True\n",
    "else: \n",
    "    consider_year = False\n",
    "    consider_month = False\n",
    "    consider_day = False\n",
    "\n",
    "\n",
    "\n",
    "scaler_X = MinMaxScaler(feature_range=(0, 1)).fit(X_MLP)\n",
    "\n",
    "scaler_y = MinMaxScaler(feature_range=(0, 1)).fit(y_MLP)\n",
    "\n",
    "X_scaled = pd.DataFrame(scaler_X.transform(X_MLP[X_MLP.columns]), columns=X_MLP.columns)\n",
    "y_scaled = pd.DataFrame(scaler_y.transform(y_MLP[y_MLP.columns]), columns=y_MLP.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec623efb-eac3-4f70-85d3-93a9a428f470",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled=X_scaled.dropna().reset_index(drop=True)\n",
    "y_scaled=y_scaled.dropna().reset_index(drop=True)\n",
    "\n",
    "# Onde fui buscar algumas coisas:\n",
    "# https://www.projectpro.io/recipes/find-optimal-parameters-using-gridsearchcv\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=RANDOM_SEED)\n",
    "\n",
    "number_columns = len(X_scaled.columns)#.count()\n",
    "#print(\"Número de colunas: \", number_columns)\n",
    "\n",
    "def build_model(activation='relu', learning_rate=0.01):\n",
    "#create a sequential model (with three Layers - Last one is the output)\n",
    "    model = Sequential()\n",
    "    model.add(Dense(number_columns, input_dim=number_columns, activation=activation))\n",
    "    model.add(Dense(6, activation=activation))\n",
    "    model.add(Dense(1, activation='relu'))\n",
    "  \n",
    "     #compile the model\n",
    "    #Define the Loss function, the otimizer and metrics to be used\n",
    "    model. compile(\n",
    "        loss = 'mae',\n",
    "        optimizer = tf.optimizers.Adam(learning_rate),\n",
    "        metrics = ['mae', 'mse'])\n",
    "    return model\n",
    "\n",
    "model = build_model()\n",
    "model.summary()\n",
    "''' Este é melhor, mas mais lento\n",
    "TUNING_DICT = {\n",
    "    'activation' : ['relu', 'sigmoid'],\n",
    "    'learning_rate' : [0.01, 0.001]\n",
    "}\n",
    "'''\n",
    "# = {\n",
    "#    'activation' : ['relu'],\n",
    "#    'learning_rate' : [0.01]\n",
    "#}\n",
    "\n",
    "TUNING_DICT = {'learning_rate': [0.01,0.03, 0.1],                  \n",
    "                'activation' : ['relu', 'sigmoid'],\n",
    "                 }\n",
    "\n",
    "kf = KFold(n_splits=2, shuffle=True, random_state=RANDOM_SEED)\n",
    "\n",
    "model = KerasRegressor(build_fn=build_model, epochs=15,batch_size=number_columns)\n",
    "grid_search = GridSearchCV(estimator = model,\n",
    "                            param_grid = TUNING_DICT,\n",
    "                            cv = kf,\n",
    "                            scoring = 'neg_mean_absolute_error',\n",
    "                            refit ='True',\n",
    "                            verbose = 1)\n",
    "\n",
    "grid_search.fit(X_train, y_train, validation_split=0.2)\n",
    "\n",
    "print(\" Results from Grid Search \" )\n",
    "print(grid_search)\n",
    "#print(\"\\n The best score across ALL searched params:\\n\",grid_search.best_score_)\n",
    "#print(\"\\n The best parameters across ALL searched params:\\n\",grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88aa5f0-3750-4740-9b15-346a15a56f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overfitting analisys\n",
    "\n",
    "#Our best model (remember we set refit\n",
    "best_mlp_model = grid_search.best_estimator_\n",
    "\n",
    "from livelossplot import PlotLossesKerasTF\n",
    "best_mlp_model.fit(X_train, y_train, epochs=20,\n",
    "validation_data=(X_test, y_test),\n",
    "callbacks=[PlotLossesKerasTF()], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df0dca7",
   "metadata": {},
   "source": [
    "### Análise de overfitting\n",
    "\n",
    "Pelos gráficos, vemos que o medium average error (mae) dá bastante mal, mas o mse dá bem. Como 20 epochs é um valor considerável, consideramos nesta fase melhorar o tratamento de dados, e depois verificar se há melhorias. \n",
    "Isto porque, o tratamento de dados atual para este modelo é quase com o objetivo de o modelo consiguir analisar o dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25ad198-237d-4c9b-967f-01e32218b85f",
   "metadata": {},
   "outputs": [],
   "source": [
    " #0btain predictions\n",
    "\n",
    "predictions = best_mlp_model.predict(X_test)\n",
    "\n",
    "predictions = predictions.reshape(predictions.shape[0], 1)\n",
    "predicions_unscaled = scaler_y.inverse_transform(predictions)\n",
    "predicions_unscaled[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d744c38e-6c44-45c4-b1e5-106c72871f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_unscaled = scaler_y.inverse_transform(y_test)\n",
    "\n",
    "#Visualising the actual and predicted result\n",
    "def real_predicted_viz(limit):\n",
    "    plt.figure(figsize=(14,6))\n",
    "    plt.plot(y_test_unscaled[:limit], color = 'green', label = 'Actual')\n",
    "    plt.plot (predicions_unscaled[:limit], color = 'red', label = 'Predicted')\n",
    "    plt.grid(alpha = 0.3)\n",
    "    plt.xlabel('Sample')\n",
    "    plt.ylabel ('Profit')\n",
    "    plt.title('Real vs Predicted')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "#let's Limit to 200 comparisions for better visualization\n",
    "real_predicted_viz(400)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd9685b",
   "metadata": {},
   "source": [
    "Pela análise do gráfico, podemos ver que há algumas previsões aproximadamente corretas, mas existem algumas, bem visíveis, que mostram quão errado o nosso modelo está.\n",
    "Como dito anteriormente, falta tratamento de dados. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced79ae6-6b6c-45ab-be50-611519fbc0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Best: %f using %s\" % (grid_search.best_score_, grid_search.best_params_))\n",
    "means = grid_search.cv_results_['mean_test_score']\n",
    "stds = grid_search.cv_results_['std_test_score']\n",
    "params = grid_search.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"Média: %f Desvio padrão: (%f) com: %r\" % (mean, stdev, param))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8407e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.keras.losses.mean_absolute_error(y_test_unscaled, predicions_unscaled)\n",
    "mae = tf.keras.losses.MeanAbsoluteError()\n",
    "print(\"MAE: \", mae(y_test_unscaled, predicions_unscaled).numpy())\n",
    "#mse = tf.keras.losses.mean_squared_error(y_test_unscaled, predicions_unscaled).numpy()\n",
    "mse = np.mean(np.square(y_test_unscaled - predicions_unscaled))\n",
    "print(\"MSE: \", mse)\n",
    "print(\"RMSE: \", math.sqrt(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1428dcb4",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

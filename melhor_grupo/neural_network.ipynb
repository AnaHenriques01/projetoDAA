{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b856b60-22f2-493e-a91d-83c3b4bc68d7",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5f0dbd9-a10f-4a55-9b16-d6ff09fae607",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from pandas import DatetimeIndex\n",
    "\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2edeaf-98b3-452e-a88b-8daa1d68f226",
   "metadata": {},
   "source": [
    "### Carregamento dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20203651-46ed-4cf0-bdd3-20d4e6c8ee18",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataOriginal = pd.read_excel('Global_Superstore2.xlsx', index_col=0, comment='#') \n",
    "df = dataOriginal "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790805ff-ab09-4658-b9f5-f5a04542bc49",
   "metadata": {},
   "source": [
    "### Tratamento dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a318407",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df)\n",
    "df = scaler.transform(df)\n",
    "\n",
    "pt = PowerTransformer()\n",
    "pt.fit(df)\n",
    "\n",
    "df = pt.transform(df)\n",
    "\n",
    "scaler = MinMaxScaler(feature_range = (0,1))\n",
    "df = scaler.fit_transform(df)\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e383f3a1-2934-4ebe-bc11-fb9c0f1fe0be",
   "metadata": {},
   "source": [
    "### Neurónios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "097cb478-22be-4c57-8dd2-ee4117930d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 51200 entries, 32298 to 6147\n",
      "Data columns (total 26 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   Order ID        51200 non-null  int32  \n",
      " 1   Ship Mode       51200 non-null  int32  \n",
      " 2   Customer ID     51200 non-null  int32  \n",
      " 3   Customer Name   51200 non-null  int32  \n",
      " 4   Segment         51200 non-null  int32  \n",
      " 5   City            51200 non-null  int32  \n",
      " 6   State           51200 non-null  int32  \n",
      " 7   Country         51200 non-null  int32  \n",
      " 8   Market          51200 non-null  int32  \n",
      " 9   Region          51200 non-null  int32  \n",
      " 10  Product ID      51200 non-null  int32  \n",
      " 11  Category        51200 non-null  int32  \n",
      " 12  Sub-Category    51200 non-null  int32  \n",
      " 13  Product Name    51200 non-null  int32  \n",
      " 14  Sales           51200 non-null  float64\n",
      " 15  Quantity        51200 non-null  float64\n",
      " 16  Discount        51200 non-null  int32  \n",
      " 17  Profit          51200 non-null  float64\n",
      " 18  Shipping_Cost   51200 non-null  float64\n",
      " 19  Order_Priority  51200 non-null  int32  \n",
      " 20  Year_order      51200 non-null  int64  \n",
      " 21  Year_ship       51200 non-null  int64  \n",
      " 22  Month_order     51200 non-null  int64  \n",
      " 23  Month_ship      51200 non-null  int64  \n",
      " 24  Day_order       51200 non-null  int64  \n",
      " 25  Day_ship        51200 non-null  int64  \n",
      "dtypes: float64(4), int32(16), int64(6)\n",
      "memory usage: 7.4 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#dataset_MLP = dataset.drop(columns=['Order ID', 'Customer ID', 'Customer Name', 'Product ID', 'Product Name', 'Postal Code'])\n",
    "dataset_MLP = data2\n",
    "\n",
    "#dataset_MLP = dataset_MLP.drop(['Order Priority', 'Market', \"City\", \"Segment\", \"Ship Mode\", \"Ship Date\"], axis=1)\n",
    "print(dataset_MLP.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61a945d2-3161-4a80-aa10-251be0d9fbc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: 2.10.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, KFold, train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "RANDOM_SEED = 2021\n",
    "\n",
    "print(\"Tensorflow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b207a7-b669-48dd-aa67-ce269d8dd30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X_MLP = dataset_MLP.drop('Profit', axis=1)\n",
    "y_MLP = dataset_MLP[['Profit']]\n",
    "\n",
    "small_dataset = True\n",
    "if small_dataset: \n",
    "    remove_n = 20000\n",
    "    drop_indices = np.random.choice(dataset_MLP.index, remove_n, replace=False)\n",
    "    dataset_MLP_subset = dataset_MLP.drop(drop_indices)\n",
    "    X_MLP = dataset_MLP_subset.drop('Profit', axis=1)\n",
    "    y_MLP = dataset_MLP_subset[['Profit']]\n",
    "\n",
    " \n",
    "    \n",
    "    # Convert data\n",
    "if \"Order Date\" in X_MLP:\n",
    "    consider_year = True\n",
    "    consider_month = True\n",
    "    consider_day = True\n",
    "else: \n",
    "    consider_year = False\n",
    "    consider_month = False\n",
    "    consider_day = False\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec623efb-eac3-4f70-85d3-93a9a428f470",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Onde fui buscar algumas coisas:\n",
    "# https://www.projectpro.io/recipes/find-optimal-parameters-using-gridsearchcv\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_MLP, y_MLP, test_size=0.2, random_state=RANDOM_SEED)\n",
    "\n",
    "number_columns = len(X_scaled.columns)#.count()\n",
    "#print(\"Número de colunas: \", number_columns)\n",
    "\n",
    "def build_model(activation='relu', learning_rate=0.01):\n",
    "#create a sequential model (with three Layers - Last one is the output)\n",
    "    model = Sequential()\n",
    "    model.add(Dense(number_columns, input_dim=number_columns, activation=activation))\n",
    "    model.add(Dense(6, activation=activation))\n",
    "    model.add(Dense(1, activation='relu'))\n",
    "  \n",
    "     #compile the model\n",
    "    #Define the Loss function, the otimizer and metrics to be used\n",
    "    model. compile(\n",
    "        loss = 'mae',\n",
    "        optimizer = tf.optimizers.Adam(learning_rate),\n",
    "        metrics = ['mae', 'mse'])\n",
    "    return model\n",
    "\n",
    "model = build_model()\n",
    "model.summary()\n",
    "''' Este é melhor, mas mais lento\n",
    "TUNING_DICT = {\n",
    "    'activation' : ['relu', 'sigmoid'],\n",
    "    'learning_rate' : [0.01, 0.001]\n",
    "}\n",
    "'''\n",
    "# = {\n",
    "#    'activation' : ['relu'],\n",
    "#    'learning_rate' : [0.01]\n",
    "#}\n",
    "\n",
    "TUNING_DICT = {'learning_rate': [0.01,0.03, 0.1],                  \n",
    "                'activation' : ['relu', 'sigmoid'],\n",
    "                 }\n",
    "\n",
    "kf = KFold(n_splits=2, shuffle=True, random_state=RANDOM_SEED)\n",
    "\n",
    "model = KerasRegressor(build_fn=build_model, epochs=15,batch_size=number_columns)\n",
    "grid_search = GridSearchCV(estimator = model,\n",
    "                            param_grid = TUNING_DICT,\n",
    "                            cv = kf,\n",
    "                            scoring = 'neg_mean_absolute_error',\n",
    "                            refit ='True',\n",
    "                            verbose = 1)\n",
    "\n",
    "grid_search.fit(X_train, y_train, validation_split=0.2)\n",
    "\n",
    "print(\" Results from Grid Search \" )\n",
    "print(grid_search)\n",
    "#print(\"\\n The best score across ALL searched params:\\n\",grid_search.best_score_)\n",
    "#print(\"\\n The best parameters across ALL searched params:\\n\",grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88aa5f0-3750-4740-9b15-346a15a56f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overfitting analisys\n",
    "\n",
    "#Our best model (remember we set refit\n",
    "best_mlp_model = grid_search.best_estimator_\n",
    "\n",
    "from livelossplot import PlotLossesKerasTF\n",
    "best_mlp_model.fit(X_train, y_train, epochs=20,\n",
    "validation_data=(X_test, y_test),\n",
    "callbacks=[PlotLossesKerasTF()], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df0dca7",
   "metadata": {},
   "source": [
    "### Análise de overfitting\n",
    "\n",
    "Pelos gráficos, vemos que o medium average error (mae) dá bastante mal, mas o mse dá bem. Como 20 epochs é um valor considerável, consideramos nesta fase melhorar o tratamento de dados, e depois verificar se há melhorias. \n",
    "Isto porque, o tratamento de dados atual para este modelo é quase com o objetivo de o modelo consiguir analisar o dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25ad198-237d-4c9b-967f-01e32218b85f",
   "metadata": {},
   "outputs": [],
   "source": [
    " #0btain predictions\n",
    "\n",
    "predictions = best_mlp_model.predict(X_test)\n",
    "\n",
    "\n",
    "predictions = predictions.reshape(predictions.shape[0], 1)\n",
    "predicions_unscaled = scaler_y.inverse_transform(predictions)\n",
    "predicions_unscaled[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d744c38e-6c44-45c4-b1e5-106c72871f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_unscaled = scaler_y.inverse_transform(y_test)\n",
    "\n",
    "#Visualising the actual and predicted result\n",
    "def real_predicted_viz(limit):\n",
    "    plt.figure(figsize=(14,6))\n",
    "    plt.plot(y_test_unscaled[:limit], color = 'green', label = 'Actual')\n",
    "    plt.plot (predicions_unscaled[:limit], color = 'red', label = 'Predicted')\n",
    "    plt.grid(alpha = 0.3)\n",
    "    plt.xlabel('Sample')\n",
    "    plt.ylabel ('Profit')\n",
    "    plt.title('Real vs Predicted')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "#let's Limit to 200 comparisions for better visualization\n",
    "real_predicted_viz(400)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd9685b",
   "metadata": {},
   "source": [
    "Pela análise do gráfico, podemos ver que há algumas previsões aproximadamente corretas, mas existem algumas, bem visíveis, que mostram quão errado o nosso modelo está.\n",
    "Como dito anteriormente, falta tratamento de dados. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced79ae6-6b6c-45ab-be50-611519fbc0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Best: %f using %s\" % (grid_search.best_score_, grid_search.best_params_))\n",
    "means = grid_search.cv_results_['mean_test_score']\n",
    "stds = grid_search.cv_results_['std_test_score']\n",
    "params = grid_search.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"Média: %f Desvio padrão: (%f) com: %r\" % (mean, stdev, param))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8407e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.keras.losses.mean_absolute_error(y_test_unscaled, predicions_unscaled)\n",
    "mae = tf.keras.losses.MeanAbsoluteError()\n",
    "print(\"MAE: \", mae(y_test_unscaled, predicions_unscaled).numpy())\n",
    "#mse = tf.keras.losses.mean_squared_error(y_test_unscaled, predicions_unscaled).numpy()\n",
    "mse = np.mean(np.square(y_test_unscaled - predicions_unscaled))\n",
    "print(\"MSE: \", mse)\n",
    "print(\"RMSE: \", math.sqrt(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1428dcb4",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
